# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sgj4Cqd1HWTN0Snu1QnyvhUIxn9Bgtv2
"""

!pip install prophet

from prophet import Prophet
print("Prophet successfully imported!")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet

# Load and preprocess unemployment dataset
unemployment_data = pd.read_csv("unemployment_rates.csv", skiprows=4)

# Select only year columns for melting
year_columns = [col for col in unemployment_data.columns if col.isdigit()]
unemployment_data = unemployment_data.melt(id_vars=["Country Name"], value_vars=year_columns, var_name="Year", value_name="Unemployment Rate")
unemployment_data["Year"] = pd.to_datetime(unemployment_data["Year"], format="%Y")

# Filter for a specific country (e.g., United States)
unemployment_data = unemployment_data[unemployment_data["Country Name"] == "United States"]
unemployment_data.set_index("Year", inplace=True)

# Load AI adoption dataset
ai_adoption_data = pd.read_csv("ai_adoption_data.csv", encoding="ISO-8859-1")

# Extract relevant data for AI adoption trends
ai_adoption_data = ai_adoption_data[["Country", "Ai and ML(Popularity)"]].dropna()
ai_adoption_data = ai_adoption_data.rename(columns={"Ai and ML(Popularity)": "AI Popularity"})

# Assume AI Popularity follows a yearly trend and assign synthetic years for merging
ai_adoption_data["Year"] = pd.date_range(start="2000", periods=len(ai_adoption_data), freq="Y")
ai_adoption_data.set_index("Year", inplace=True)

# Merge datasets on time
merged_data = unemployment_data.join(ai_adoption_data, how='outer')

print(merged_data.head())
print(merged_data.dtypes)
print(merged_data.isnull().sum())

print("Unemployment Data Years:", unemployment_data.index.unique())
print("AI Adoption Data Years:", ai_adoption_data.index.unique())

ai_adoption_data["Year"] = ai_adoption_data.index.year
ai_adoption_data = ai_adoption_data.reset_index(drop=True)  # Reset index before merging

unemployment_data = unemployment_data.reset_index()
unemployment_data["Year"] = unemployment_data["Year"].dt.year

print("Unemployment Data Years:", unemployment_data.index.unique())
print("AI Adoption Data Years:", ai_adoption_data.index.unique())

# Convert Year to datetime in unemployment data
unemployment_data["Year"] = pd.to_datetime(unemployment_data["Year"], format="%Y")

# Convert AI adoption data Year from full date to just the year
ai_adoption_data.index = ai_adoption_data.index.year

if merged_data.empty:
    print("Merged data is empty. Check the dataset processing steps.")
else:
    print("Data available for plotting.")

print("Merged Data Preview:")
print(merged_data.head())

print("Merged Data Shape:", merged_data.shape)

# Load AI job market dataset
ai_job_market_data = pd.read_csv("ai_job_market_insights.csv", encoding="ISO-8859-1")

# Analyze AI adoption levels and automation risks
ai_adoption_risk = ai_job_market_data.groupby("AI_Adoption_Level")["Automation_Risk"].value_counts().unstack()
ai_adoption_risk.plot(kind="bar", stacked=True, figsize=(10, 6), colormap="coolwarm")
plt.title("AI Adoption Level vs. Automation Risk")
plt.xlabel("AI Adoption Level")
plt.ylabel("Count")
plt.legend(title="Automation Risk")
plt.show()

# Exploratory Data Analysis
plt.figure(figsize=(12, 6))
sns.lineplot(data=merged_data, x="Year", y="Unemployment Rate", label="Unemployment Rate")
sns.lineplot(data=merged_data, x="Year", y="AI Popularity", label="AI Popularity")
plt.title("Unemployment Rate vs. AI Popularity Over Time")
plt.xlabel("Year")
plt.ylabel("Rate / Popularity")
plt.legend()
plt.show()

print("Industry Data Columns:", industry_data.columns)

# Industry-wise Impact
industry_data = pd.read_csv("ai_job_market_insights.csv")
plt.figure(figsize=(10, 5))
sns.barplot(data=ai_job_market_data, x="Industry", y="Automation_Risk", palette="Reds")
plt.xticks(rotation=45)
plt.title("Industry-wise AI Adoption and Job Loss Risk")
plt.xlabel("Industry")
plt.ylabel("Automation Risk")
plt.show()

# Time Series Decomposition
result = seasonal_decompose(merged_data['Unemployment Rate'], model='multiplicative', period=12)
result.plot()
plt.show()

# ARIMA Model for Forecasting
model = ARIMA(merged_data['Unemployment Rate'], order=(5,1,0))
model_fit = model.fit()
forecast = model_fit.forecast(steps=5)
print("Future Unemployment Rate Predictions:")
print(forecast)

# Prophet Model for Future Predictions
df_prophet = merged_data[['Year', 'Unemployment Rate']].rename(columns={'Year': 'ds', 'Unemployment Rate': 'y'})
prophet = Prophet()
prophet.fit(df_prophet)
future = prophet.make_future_dataframe(periods=5, freq='Y')
forecast = prophet.predict(future)
prophet.plot(forecast)
plt.show()

print("Project Completed: AI and Unemployment Analysis")